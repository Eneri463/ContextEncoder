{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6294f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from random import randint\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78cf20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 6\n",
    "imageHeight = 256\n",
    "imageWidth = 256\n",
    "imageChannels = 3\n",
    "cropHeight = 64\n",
    "cropWidth = 64 \n",
    "epochs = 30\n",
    "pp = 30000 # количество изображений, которые будут использоваться для обучения/тестирования\n",
    "pathM = 'data/masks'\n",
    "pathMT = 'data/masks_test'\n",
    "pathI = 'data/CelebA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c486e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# класс, создающий контекстный кодировщик и обучающий его\n",
    "class ContextEncoder:\n",
    "    \n",
    "    def __init__(self,  trainData, testData, epochs, batchSize, imageHeight = 128, imageWidth = 128, imageChannels = 3, cropHeight = 64, cropWidth = 64):\n",
    "        # ----- гиперпараметры обучения\n",
    "        self.epochs = epochs        # количество эпох\n",
    "        self.batchSize = batchSize      # размер одного батча\n",
    "        self.imageHeight = imageHeight   # высота изображения\n",
    "        self.imageWidth = imageWidth    # ширина изображения\n",
    "        self.cropHeight = cropHeight     # высота восстанавливаемой области\n",
    "        self.cropWidth = cropWidth      # ширина восстанавливаемой области\n",
    "        self.imageChannels = imageChannels   # количество каналов в изображении\n",
    "        \n",
    "        # ----- объекты и данные, используемые при обучении\n",
    "        \n",
    "        # данные для оубчения\n",
    "        self.trainData = trainData\n",
    "        self.testData = testData\n",
    "        # исходное изображение с повреждённой областью внутри\n",
    "        self.inputs =  tf.placeholder(tf.float32, [self.batchSize, self.imageHeight, self.imageWidth, self.imageChannels])\n",
    "        # оригинальная восстанавливаемая часть\n",
    "        self.cropInputs = tf.placeholder(tf.float32, [self.batchSize, self.cropHeight, self.cropWidth, self.imageChannels])\n",
    "        # генератор\n",
    "        generator = GEN(\"generator\")\n",
    "        # дискриминатор\n",
    "        discriminator = DIS(\"discriminator\")\n",
    "        \n",
    "        self.cropFake = generator(self.inputs)\n",
    "        self.disTrue = discriminator(self.cropInputs)\n",
    "        self.disFake = discriminator(self.cropFake)\n",
    "        \n",
    "        self.disLoss = -tf.reduce_mean(tf.log(self.disTrue + 1e-5) + tf.log(1 - self.disFake + 1e-5))\n",
    "        self.genLoss = -tf.reduce_mean(tf.log(self.disFake + 1e-5)) + 100*tf.reduce_mean(tf.reduce_sum(tf.square(self.cropInputs - self.cropFake), [1, 2, 3]))\n",
    "        \n",
    "        self.disOptimizer = tf.train.AdamOptimizer(2e-4).minimize(self.disLoss, var_list=discriminator.get_var())\n",
    "        self.genOptimizer = tf.train.AdamOptimizer(2e-4).minimize(self.genLoss, var_list=generator.get_var())\n",
    "        \n",
    "        self.costDis = tf.summary.scalar(\"disLoss\", self.disLoss)\n",
    "        self.costGen = tf.summary.scalar(\"genLoss\", self.genLoss)\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.writerTest = tf.summary.FileWriter(\"./logs/test\")\n",
    "        self.writerTrain = tf.summary.FileWriter(\"./logs/train\")\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    # -------------------------------------------------------------- обучение\n",
    "    def train(self, i = 0, whenSave = 1):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.writerTrain.add_graph(self.sess.graph)\n",
    "        self.writerTest.add_graph(self.sess.graph)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            im = []\n",
    "            im2 = []\n",
    "            \n",
    "            # ----- шаг обучения\n",
    "            for numberBatch in range(len(self.trainData)):\n",
    "                \n",
    "                im, crop = self.trainData[numberBatch]\n",
    "                \n",
    "                self.sess.run(self.disOptimizer, feed_dict={self.inputs: im, self.cropInputs: crop})\n",
    "                self.sess.run(self.genOptimizer, feed_dict={self.inputs: im, self.cropInputs: crop})\n",
    "            \n",
    "            # ----- вывод промежуточных результатов:\n",
    "            im2, crop2 = self.testData[0]\n",
    "\n",
    "            summaryTrain, resLossDTrain, resLossGTrain = self.sess.run([self.merged, self.disLoss, self.genLoss], feed_dict={self.inputs: im, self.cropInputs: crop})\n",
    "\n",
    "            summaryTest, resLossDTest, resLossGTest = self.sess.run([self.merged, self.disLoss, self.genLoss], feed_dict={self.inputs: im2, self.cropInputs: crop2})\n",
    "\n",
    "            self.writerTrain.add_summary(summaryTrain, i)\n",
    "            self.writerTest.add_summary(summaryTest, i)\n",
    "\n",
    "            print(\"Итерация \" + str(i) + \". D loss = \" + str(resLossDTrain) + \", D loss Test = \" + str(resLossDTest) + \", G loss = \" + str(resLossGTrain) + \", G loss Test = \" + str(resLossGTest) + \".\")\n",
    "\n",
    "            # ----- сохраняем параметры нейросети каждые whenSave эпох\n",
    "            if (epoch + 1) % whenSave == 0:\n",
    "\n",
    "                resImage = self.sess.run([self.cropFake], feed_dict={self.inputs: im})\n",
    "                Image.fromarray(np.uint8(resImage[0][0]*255)).save(\"./Results//\" + str(i) + \".jpg\")\n",
    "                self.saver.save(self.sess, \"./save_para//para.ckpt\")\n",
    "\n",
    "            self.trainData.on_epoch_end()\n",
    "            self.testData.on_epoch_end()\n",
    "            \n",
    "    # -------------------------------------------------------------- \n",
    "    # восстановление данных        \n",
    "    def restoreModel(self, pathMeta, path):\n",
    "\n",
    "        self.saver = tf.train.import_meta_graph(pathMeta)\n",
    "        self.saver.restore(self.sess, tf.train.latest_checkpoint(path))\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------- \n",
    "    # использование готовой модели для восстановления изображения:\n",
    "    def useModel(self, image):\n",
    "        \n",
    "        resImage = self.sess.run([self.outputs], feed_dict={self.damagedInputs: image, self.masks: mask})\n",
    "        Image.fromarray(np.uint8(resImage[0][0]*255)).save(\"./Results.jpg\")\n",
    "        print(\"Результат сохранен\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79db8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# реализация слоёв нейронной сети \n",
    "\n",
    "# ----- свёртка\n",
    "# подаётся: \n",
    "# название операции, входные данные, количество фильтров, размер ядра, шаг stride, тип паддинга\n",
    "def conv2D(name, inputs, filters, kSize, stride, padding):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        W = tf.get_variable(\"W\", shape=[kSize, kSize, inputs.shape[-1], filters], initializer=tf.random_normal_initializer(0., 0.005))\n",
    "        b = tf.get_variable(\"b\", shape=[filters], initializer=tf.constant_initializer(0.))\n",
    "        \n",
    "        return  tf.math.add(tf.nn.conv2d(inputs, W, [1, stride, stride, 1], padding), b)\n",
    "\n",
    "# ----- обратная свёртка\n",
    "# подаётся: \n",
    "# название операции, входные данные, количество фильтров, размер ядра, шаг stride, тип паддинга\n",
    "def unconv2D(name, inputs, filters, kSize, stride, padding, r = 2):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        w = tf.get_variable(\"W\", shape=[kSize, kSize, filters, int(inputs.shape[-1])], initializer=tf.random_normal_initializer(0., 0.005))\n",
    "        b = tf.get_variable(\"b\", shape=[filters], initializer=tf.constant_initializer(0.))\n",
    "        \n",
    "        B = tf.shape(inputs)[0]\n",
    "        H = int(inputs.shape[1])\n",
    "        W = int(inputs.shape[2])\n",
    "        \n",
    "        return  tf.math.add(tf.nn.conv2d_transpose(inputs, w, [B, H*r, W*r, filters], [1, stride, stride, 1], padding), b)\n",
    "\n",
    "# ----- полносвязный слой / вектор \n",
    "def fullyConnected(name, inputs, filters):\n",
    "    \n",
    "    inputs = tf.layers.flatten(inputs)\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        W = tf.get_variable(\"W\", [int(inputs.shape[-1]), filters], initializer=tf.random_normal_initializer(0., 0.005))\n",
    "        b = tf.get_variable(\"b\", [filters], initializer=tf.constant_initializer(0.))\n",
    "    \n",
    "        return tf.math.add(tf.matmul(inputs, W), b)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# класс генератора\n",
    "class GEN:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            # ----- блок кодировщика (encoder)\n",
    "            model = tf.nn.leaky_relu(conv2D(\"conv1\", inputs, 64, 4, 2, \"SAME\"))\n",
    "            model = tf.nn.leaky_relu(conv2D(\"conv2\", model, 64, 4, 2, \"SAME\"))\n",
    "            model = tf.nn.leaky_relu(conv2D(\"conv3\", model, 128, 4, 2, \"SAME\"))\n",
    "            model = tf.nn.leaky_relu(conv2D(\"conv4\", model, 256, 4, 2, \"SAME\"))\n",
    "            model = tf.nn.leaky_relu(conv2D(\"conv5\", model, 512, 4, 2, \"SAME\"))\n",
    "            model = tf.nn.leaky_relu(conv2D(\"conv6\", model, 4000, 4, 2, \"VALID\"))\n",
    "            \n",
    "            \n",
    "            # ----- блок декодировщика (decoder)\n",
    "            model = tf.nn.relu(unconv2D(\"unconv1\", model, 512, 4, 2, \"VALID\", 4))\n",
    "            model = tf.nn.relu(unconv2D(\"unconv2\", model, 256, 4, 2, \"SAME\"))\n",
    "            model = tf.nn.relu(unconv2D(\"unconv3\", model, 128, 4, 2, \"SAME\"))\n",
    "            model = tf.nn.relu(unconv2D(\"unconv4\", model, 64, 4, 2, \"SAME\"))\n",
    "            model = tf.nn.tanh(unconv2D(\"unconv5\", model, 3, 4, 2, \"SAME\"))\n",
    "    \n",
    "            return model\n",
    "\n",
    "    def get_var(self):\n",
    "        return  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.name)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# класс дискриминатора\n",
    "class DIS:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        \n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            # ----- блок кодировщика (encoder)\n",
    "            model = tf.nn.leaky_relu(conv2D(\"conv1\", inputs, 64, 4, 2, \"SAME\"))\n",
    "            model = tf.nn.leaky_relu(conv2D(\"conv3\", model, 128, 4, 2, \"SAME\"))\n",
    "            model = tf.nn.leaky_relu(conv2D(\"conv4\", model, 256, 4, 2, \"SAME\"))\n",
    "            model = tf.nn.leaky_relu(conv2D(\"conv5\", model, 512, 4, 2, \"SAME\"))\n",
    "            model = tf.sigmoid(fullyConnected(\"fc\", model, 1))\n",
    "    \n",
    "            return model\n",
    "\n",
    "    def get_var(self):\n",
    "        return  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d73b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# класс, генерирующий тренировочные данные\n",
    "class createAugment():\n",
    "    \n",
    "    # --\n",
    "    # инициализация объекта класса\n",
    "    def __init__(self, X, batch_size=32, dim=(128, 128), n_channels=3, crop_size = 64):\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.n_channels = n_channels\n",
    "        self.crop_size = crop_size\n",
    "        self.indexes = []\n",
    "        self.X = X\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    # --\n",
    "    # результат: максимальное количество батчей в одной эпохе\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.X) / self.batch_size))\n",
    "    \n",
    "    # --\n",
    "    # результат: взятие батча с заданным номером (индексом)\n",
    "    def __getitem__(self, index):\n",
    "        indexs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X_inputs, y_output = self.data_generation(indexs)\n",
    "        return X_inputs, y_output\n",
    "    \n",
    "    # --\n",
    "    # функция, повторяющаяся в конце каждой эпохи\n",
    "    # результат: новая совокупность индексов изображений для очередного батча\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "        np.random.shuffle(self.indexes)\n",
    "    \n",
    "    # --\n",
    "    # результат: батч данных, включающий в себя \n",
    "    # маскированное изображение, маску и исходное изображение\n",
    "    def data_generation(self, idxs):\n",
    "        \n",
    "        # создаём пустые массивы для маскированных изображений батча, их масок \n",
    "        # и исходных изображений соответственно\n",
    "        masked_images = np.empty((self.batch_size, self.dim[0], self.dim[1], self.n_channels)) # Masked image\n",
    "        y_batch = np.empty((self.batch_size, 64, 64, 3)) # Original image\n",
    "        \n",
    "        for i, idx in enumerate(idxs):\n",
    "            \n",
    "            image_copy = self.X[idx].copy()\n",
    "            masked_image, y_new  = self.createMask(image_copy)\n",
    "            masked_images[i,] = masked_image/255\n",
    "            y_batch[i] = y_new/255\n",
    "            \n",
    "        return masked_images, y_batch\n",
    "\n",
    "    def createMask(self, image):\n",
    "        \n",
    "        x1 = randint(1, 64)\n",
    "        y1 = randint(1, 64)\n",
    "        x2 = x1 + 64\n",
    "        y2 = y1 + 64\n",
    "        \n",
    "        image2 = image.copy()\n",
    "        cropped_image = image2[y1:y2, x1:x2]\n",
    "        image = cv2.rectangle(image, (x1, y1), (x2, y2), (255, 255, 255), -1)\n",
    "        \n",
    "        return image, cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3443c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = 20000\n",
    "paths = os.listdir('data/faces') # список файлов датасета\n",
    "image = np.empty((pp, 128, 128, 3), dtype='uint8')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for path in paths:\n",
    "    img = Image.open(os.path.join('data/faces', path))\n",
    "    img = img.resize((128,128))\n",
    "    b = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    image[i] = b\n",
    "    i = i+1\n",
    "    if i == pp:\n",
    "        break\n",
    "        \n",
    "trainData = createAugment(image[0:int(pp*0.9)], 20)\n",
    "testData = createAugment(image[int(pp*0.9):], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb42ec8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eneri\\anaconda3\\envs\\pleaseWork\\lib\\site-packages\\keras\\legacy_tf_layers\\core.py:513: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n",
      "C:\\Users\\Eneri\\anaconda3\\envs\\pleaseWork\\lib\\site-packages\\keras\\engine\\base_layer_v1.py:1676: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "network = ContextEncoder(trainData, testData, epochs, batchSize, imageHeight, imageWidth, imageChannels, cropHeight, cropWidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1027d30b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итерация 0. D loss = 1.352983, D loss Test = 1.3713856, G loss = 318289.28, G loss Test = 146178.48.\n",
      "Итерация 1. D loss = 1.0305067, D loss Test = 1.2092521, G loss = 319920.9, G loss Test = 168446.81.\n",
      "Итерация 2. D loss = 3.3459733, D loss Test = 2.4694295, G loss = 196811.05, G loss Test = 110618.14.\n",
      "Итерация 3. D loss = 1.3323561, D loss Test = 1.3659166, G loss = 160783.45, G loss Test = 72494.805.\n",
      "Итерация 4. D loss = 1.3959322, D loss Test = 1.3943266, G loss = 146971.78, G loss Test = 99553.56.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\matrix_operations.cpp:67: error: (-215:Assertion failed) src[i].dims <= 2 && src[i].rows == src[0].rows && src[i].type() == src[0].type() in function 'cv::hconcat'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 88\u001b[0m, in \u001b[0;36mContextEncoder.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     87\u001b[0m     resImage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msess\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcropFake], feed_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs: im})\n\u001b[1;32m---> 88\u001b[0m     p2 \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mim2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresImage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     Image\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39muint8(p2))\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Results11.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m     Image\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39muint8(resImage[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m255\u001b[39m))\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Results//\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\matrix_operations.cpp:67: error: (-215:Assertion failed) src[i].dims <= 2 && src[i].rows == src[0].rows && src[i].type() == src[0].type() in function 'cv::hconcat'\n"
     ]
    }
   ],
   "source": [
    "network.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb7981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e09fbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
